from itertools import chain
from paintstorch.data import PaintsTorchDataset, Sample
from paintstorch.network import (
    Discriminator, Generator, Guide, Illustration2Vec, VGG16Features,
)
from torch.cuda.amp import autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from typing import Callable, List

import argparse
import datetime
import numpy as np
import os
import torch
import torch.nn as nn
import torch.nn.functional as F


def apply(*objects: List, transform: Callable) -> List:
    return [transform(o) for o in objects]


def parallel(*objects: List) -> List:
    return apply(*objects, transform=nn.DataParallel)


def cuda(*objects: List) -> List:
    return apply(*objects, transform=lambda x: x.cuda())


def clone_unsqueeze_cuda(*objects: List) -> List:
    return apply(*objects, transform=lambda x: x.clone().unsqueeze(0).cuda())


def squeeze_permute_cpu_np(*objects: List) -> List:
    return apply(
        *objects,
        transform=lambda x: x.squeeze(0).permute((1, 2, 0)).cpu().numpy(),
    )


class GradientPenalty(nn.Module):
    def __init__(self, 位: float) -> None:
        super(GradientPenalty, self).__init__()
        self.位 = 位

    def forward(
        self,
        D: nn.Module,
        real: torch.Tensor,
        fake: torch.Tensor,
        f: torch.Tensor,
    ) -> torch.Tensor:
        伪 = torch.rand((real.size(0), 1, 1, 1), device=real.device)
        t = 伪 * real.data - (1 - 伪) * fake.data
        t.requires_grad = True
        Dt = D(t, f)

        grads = torch.autograd.grad(
            outputs=Dt,
            inputs=t,
            grad_outputs=torch.ones_like(Dt),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]

        grads = grads.view(grads.size(0), -1)
        return self.位 * ((grads.norm(2, dim=1) - 1) ** 2).mean()


@torch.no_grad()
@autocast()
def test(
    F1: nn.Module, G: nn.Module, C: nn.Module, sample: Sample, is_guide: bool,
) -> np.ndarray:
    y_, x_, h_, c_ = sample
    y, x, h, c = clone_unsqueeze_cuda(y_, x_, h_, c_)
    h_ = F.interpolate(h.clone(), size=(128, 128))
    f = F1(x)

    mask = x[:, -1].unsqueeze(1)
    fake, guide, residuals = G(x, h_, f)
    fake = x[:, :3] * (1 - mask) + fake.clip(-1, 1) * mask
    
    if is_guide:
        guide = c * (1 - mask) + C(guide, residuals).clip(-1, 1) * mask
    else:
        guide = c

    x, h, c, y, fake, guide = squeeze_permute_cpu_np(x, h, c, y, fake, guide)
    x, h, c = x[..., :3], h[..., :3], c[..., :3]
    
    img = (np.hstack([x, h, c, guide, y, fake]) * 0.5) + 0.5
    img = (img * 255).astype(np.uint8)
    return img


parser = argparse.ArgumentParser()
parser.add_argument("--features",      type=int, default=32)
parser.add_argument("--epochs",        type=int, default=40)
parser.add_argument("--batch_size",    type=int, default=4)
parser.add_argument("--num_workers",   type=int, default=4)
parser.add_argument("--dataset",       type=str, default="./dataset")
parser.add_argument("--tensorboard",   type=str, default="./experiments")
parser.add_argument("--checkpoint",    type=str, default="./")
parser.add_argument("--guide",         action="store_true")
parser.add_argument("--parallel",      action="store_true")
parser.add_argument("--bn",            action="store_true")
args, _ = parser.parse_known_args()

epochs = args.epochs
batch_size = args.batch_size
features = args.features

伪 = 1e-4
尾 = 0.5, 0.9
蔚_drift = 1e-3
位1 = 1e-4
位2 = 10
纬 = 0.1
纬_step = epochs // 2

images = args.dataset
preps = f"{(images[:-1] if images.endswith('/') else images)}_preprocessed"

skeletonizer = "./models/skeletonizer.ts"
timestamp = datetime.datetime.now().timestamp()
log_dir = os.path.join(args.tensorboard, f"paintstorch_2_{timestamp}")

testset = PaintsTorchDataset((images, preps), skeletonizer, train=False)
sample_no_hints = testset[0]

dataset = PaintsTorchDataset((images, preps), skeletonizer, train=True)
sample_hints = dataset[0]

loader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=args.num_workers,
    pin_memory=True,
    drop_last=True,
)

G = Generator(features, bn=args.bn)
D = Discriminator(features, bn=args.bn)
C = Guide(features, bn=args.bn) if args.guide else nn.Identity()

fake = torch.zeros((1, 4, 512, 512))
F1 = torch.jit.trace(Illustration2Vec("./models/i2v.pth").eval(), fake)
F2 = torch.jit.trace(VGG16Features().eval(), fake)

MSE = nn.MSELoss()
L1 = nn.L1Loss()
GP = GradientPenalty(位2)

if args.parallel:
    G, D, C, F1, F2, MSE, GP = parallel(G, D, C, F1, F2, MSE, GP)
G, D, C, F1, F2, MSE, GP = cuda(G, D, C, F1, F2, MSE, GP)

optim = lambda x: AdamW(x, lr=伪, betas=尾)
optimG = optim(chain(G.parameters(), C.parameters()))
optimD = optim(D.parameters())

params = {"step_size": 纬_step, "gamma": 纬}
schedulerG = StepLR(optimG, **params)
schedulerD = StepLR(optimD, **params)

writer = SummaryWriter(log_dir=log_dir)

for epoch in tqdm(range(args.epochs), desc="Epoch"):
    total_D_fake = 0.0
    total_D_real = 0.0
    total_G_adv = 0.0
    total_G_content = 0.0
    total_G_guide = 0.0
    total_loss = 0.0

    G.train()
    D.train()
    with tqdm(loader, desc="Batch") as pbar:
        for y, x, h, c in pbar:
            h = F.interpolate(h, size=(128, 128))
            y, x, h, c = cuda(y, x, h, c)

            optimG.zero_grad()
            optimD.zero_grad()

            with torch.no_grad():
                f = F1(x)
            mask = x[:, -1].unsqueeze(1)
            fake, guide, residuals = G(x, h, f)
            fake = x[:, :3] * (1 - mask) + fake * mask
            if args.guide:
                guide = c * (1 - mask) + C(guide, residuals) * mask

            _adv = 位1 * torch.relu(1 - D(fake, f)).mean()
            _content = MSE(F2(fake), F2(y)).mean()
            _guide = L1(guide, c).mean() if args.guide else 0
            _G = _adv + _content + _guide
            _G.backward()
            optimG.step()

            total_G_adv += _adv.item() / len(loader)
            total_G_content += _content.item() / len(loader)
            total_G_guide += _guide.item() / len(loader) if args.guide else 0

            optimD.zero_grad()

            _fake = torch.relu(1 + D(fake.detach(), f)).mean()
            _real = torch.relu(1 - D(y, f)).mean()
            _real = _real + 蔚_drift * _real ** 2
            _p = GP(D, y, fake, f).mean()
            _D = _fake + _real + _p
            _D.backward()
            optimD.step()
            
            total_D_fake += _fake.item() / len(loader)
            total_D_real += _real.item() / len(loader)

            total_D = total_D_fake + total_D_real
            total_G = total_G_adv + total_G_content + total_G_guide
            total_loss = total_D + total_G

            pbar.set_postfix(
                D_fake=total_D_fake,
                D_real=total_D_real,
                G_adv=total_G_adv,
                G_content=total_G_content,
                G_guide=total_G_guide,
                total=total_loss,
            )

    schedulerG.step()
    schedulerD.step()

    G.eval()
    nh_img = test(F1, G, C, sample_no_hints, is_guide=args.guide)
    h_img = test(F1, G, C, sample_hints, is_guide=args.guide)

    step = epoch + 1
    
    writer.add_scalar("Loss/D/fake", total_D_fake, step)
    writer.add_scalar("Loss/D/real", total_D_real, step)
    writer.add_scalar("Loss/G/adv", total_G_adv, step)
    writer.add_scalar("Loss/G/content", total_G_content, step)
    writer.add_scalar("Loss/G/guide", total_G_guide, step)
    writer.add_scalar("Loss/total", total_loss, step)

    writer.add_scalar("Hypermarameter/lr", optimG.param_groups[0]['lr'], step)
    
    writer.add_image("Image/no_hints", nh_img, step, dataformats="HWC")
    writer.add_image("Image/hints", h_img, step, dataformats="HWC")

    torch.save(
        G.state_dict(),
        os.path.join(args.checkpoint, f"checkpoint_{epoch:02d}.pth"),
    )
